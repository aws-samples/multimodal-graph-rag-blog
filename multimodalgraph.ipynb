{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19b8438-8711-4221-a354-e9c365884c29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet boto3==1.34.162 matplotlib scikit-learn tqdm langchain langchain-community langchain-experimental langchain_aws nest_asyncio json-repair awscli numpy pandas gremlinpython config streamlit\n",
    "%pip install --upgrade --quiet PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677146f4-9a42-4a6e-92cf-aee77281a0cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gremlinpython in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.10/site-packages (3.7.3)\n",
      "Requirement already satisfied: nest-asyncio in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.10/site-packages (from gremlinpython) (1.6.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.0 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.10/site-packages (from gremlinpython) (3.10.5)\n",
      "Requirement already satisfied: aenum<4.0.0,>=1.4.5 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.10/site-packages (from gremlinpython) (3.1.15)\n",
      "Requirement already satisfied: isodate<1.0.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.10/site-packages (from gremlinpython) (0.6.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.3 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.10/site-packages (from gremlinpython) (4.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->gremlinpython) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->gremlinpython) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->gremlinpython) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->gremlinpython) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->gremlinpython) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.0->gremlinpython) (1.11.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.10/site-packages (from isodate<1.0.0,>=0.6.0->gremlinpython) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp<4.0.0,>=3.8.0->gremlinpython) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.0 in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>=3.8.0->gremlinpython) (3.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nest_asyncio in /home/ec2-user/anaconda3/envs/JupyterSystemEnv/lib/python3.10/site-packages (1.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gremlinpython\n",
    "%pip install nest_asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42448da6-d919-4d94-aaf2-b0c819353298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.graphs.neptune_graph import NeptuneGraph\n",
    "import config as cf\n",
    "import os\n",
    "import base64\n",
    "from gremlin_python.driver import client, serializer\n",
    "# Additional libraries for data processing\n",
    "import pandas as pd\n",
    "\n",
    "neptune_endpoint = 'neptunedbcluster-vugbwk7upvfa.cluster-cdpzhcjwte6u.us-east-1.neptune.amazonaws.com'\n",
    "neptune_port = '8182'\n",
    "\n",
    "# Create a Neptune client\n",
    "neptune_client = client.Client(\n",
    "    f'wss://{neptune_endpoint}:{neptune_port}/gremlin',\n",
    "    'g',\n",
    "    message_serializer=serializer.GraphSONSerializersV2d0()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153905d8-6970-4f9e-942f-204275adffd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompting = \"\"\"# Knowledge Graph Instructions for Claude 3.5\n",
    "## 1. Overview\n",
    "You are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph. You are a data scientist working for a company that is building a graph database. Your task is to extract information from data and convert it into three words with meaning Source_Node, Relationship, Target_Node. The first and third words are to be used as nodes and the second word is to be used as relationship. So, maintain consistency in spelling and capital letter such that node with same name is formed only once later.\n",
    "- **Nodes** represent entities and concepts. They're akin to Wikipedia nodes.\n",
    "- The aim is to achieve simplicity and clarity in the knowledge graph, making it accessible for a vast audience.\n",
    "## 2. Labeling Nodes\n",
    "- **Consistency**: Ensure you use basic or elementary types for node labels.\n",
    "  - For example, when you identify an entity representing a person, always label it as **\"person\"**. Avoid using more specific terms like \"mathematician\" or \"scientist\".\n",
    "- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\n",
    "## 3. Handling Numerical Data and Dates\n",
    "- Numerical data, like age or other related information, should be incorporated as attributes or properties of the respective nodes.\n",
    "- **No Separate Nodes for Dates/Numbers**: Do not create separate nodes for dates or numerical values. Always attach them as attributes or properties of nodes.\n",
    "- **Property Format**: Properties must be in a key-value format.\n",
    "- **Quotation Marks**: Never use escaped single or double quotes within property values.\n",
    "- **Naming Convention**: Use camelCase for property keys, e.g., `birthDate`.\n",
    "## 4. Coreference Resolution\n",
    "- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\n",
    "If an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"), \n",
    "always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \"John Doe\" as the entity ID.  \n",
    "Remember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial. \n",
    "## 5. Strict Compliance\n",
    "Adhere to the rules strictly. Non-compliance will result in termination.\n",
    "\n",
    "<rules>\n",
    "Only return this list and nothing else. \n",
    "No extra words or sentences.\n",
    "Do not add \"Here is the list of key points from the document:\"\n",
    "</rules>\n",
    "\n",
    "Example:\n",
    "Data: Alice lawyer and is 25 years old and Bob is her roommate since 2001. Bob works as a journalist. Alice owns a the webpage www.alice.com and Bob owns the webpage www.bob.com.\n",
    "\n",
    "List created is given below. \n",
    "\n",
    "\\n\\nAssistant:\n",
    "['Alice~~is~~Lawyer', 'Alice~~age~~25years', 'Alice~~roommate~~Bob', 'Bob~~is~~Journalist', 'Alice~~owns~~www.alice.com', 'Bob~~owns~~www.bob.com']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompting_image = \"\"\"# Knowledge Graph Instructions for Claude 3.5\n",
    "## 1. Overview\n",
    "You are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph. You are a data scientist working for a company that is building a graph database. Your task is to extract information from data and convert it into three words with meaning Source_Node, Relationship, Target_Node. The first and third words are to be used as nodes and the second word is to be used as relationship. So, maintain consistency in spelling and capital letter such that node with same name is formed only once later.\n",
    "- **Nodes** represent entities and concepts. They're akin to Wikipedia nodes.\n",
    "- The aim is to achieve simplicity and clarity in the knowledge graph, making it accessible for a vast audience.\n",
    "## 2. Labeling Nodes\n",
    "- **Consistency**: Ensure you use basic or elementary types for node labels.\n",
    "  - For example, when you identify an entity representing a person, always label it as **\"person\"**. Avoid using more specific terms like \"mathematician\" or \"scientist\".\n",
    "- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\n",
    "## 3. Handling Numerical Data and Dates\n",
    "- Numerical data, like age or other related information, should be incorporated as attributes or properties of the respective nodes.\n",
    "- **No Separate Nodes for Dates/Numbers**: Do not create separate nodes for dates or numerical values. Always attach them as attributes or properties of nodes.\n",
    "- **Property Format**: Properties must be in a key-value format.\n",
    "- **Quotation Marks**: Never use escaped single or double quotes within property values.\n",
    "- **Naming Convention**: Use camelCase for property keys, e.g., `birthDate`.\n",
    "## 4. Coreference Resolution\n",
    "- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\n",
    "If an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"), \n",
    "always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \"John Doe\" as the entity ID.  \n",
    "Remember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial. \n",
    "## 5. Strict Compliance\n",
    "Adhere to the rules strictly. Non-compliance will result in termination.\n",
    "\n",
    "Example:\n",
    "Data: Alice lawyer and is 25 years old and Bob is her roommate since 2001. Bob works as a journalist. Alice owns a the webpage www.alice.com and Bob owns the webpage www.bob.com.\n",
    "\n",
    "List created is given below. Only return this list and nothing else. No extra word or sentence.\n",
    "['Alice~~is~~Lawyer', 'Alice~~age~~25years', 'Alice~~roommate~~Bob', 'Bob~~is~~Journalist', 'Alice~~owns~~www.alice.com', 'Bob~~owns~~www.bob.com']\n",
    "\n",
    "Here is the question data: {user_input}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4471b6bf-bdf2-4370-ae4c-015abc34c331",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e31ec48-16e0-401c-bf75-948e56a03255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.graphs.neptune_graph import NeptuneGraph\n",
    "import config as cf\n",
    "import os\n",
    "from gremlin_python.driver import client, serializer\n",
    "# Additional libraries for data processing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b904545f-f277-4f09-b815-992f00d011d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input= \"\"\"\n",
    "Describe the whole flow from starting to finish. Scan everything present in the image: figures, nodes, boxes, text, arrow, etc. Identify what components the flow/architecture diagram includeTry to describe one flow from one node to another through one arrow at a time using three words. The first and third word could be node and the second word could be relation. Always maintain heirarchical flow and describe the flow in steps. Do not try to skip any step in the flow. Give step-wise output without summarising. Remeber to not add anything that is not present in the picture originally.\n",
    "\n",
    "Follow these steps:-\n",
    "\n",
    "Step 1: Identify Overall Architecture\n",
    "1.1 Start: Briefly describe the type of architecture.\n",
    "1.2 Components: List the main components present in the diagram (deeply scan every box, circle, dotted box, figures and everything present in the picture.).\n",
    "\n",
    "Step 2: Follow the Flow\n",
    "2.1 Start Node: Identify the starting point of the flow (e.g., user, any external system). There can be multiple start nodes.\n",
    "2.2 Action: Describe the action or event that triggers the flow (consider and analyze each and every arrow, dotted aarow, what is present in what box, what the name of a box is, etc.).\n",
    "2.3 Intermediate / Destination Node: Identify the component where the data or action goes next (consider every figure, box, text, node, etc whereever any arrow or flow leads to). There can be multiple intermediate / destination nodes.\n",
    "\n",
    "Repeat Steps 2.2  - 2.3 for each arrow in the diagram, maintaining this format:\n",
    "    a. Action: Describe the data transformation or action happening at the current node (e.g., data validation, processing).\n",
    "    b. Relation: Describe the type of connection between nodes (e.g., sends data to, receives response from).\n",
    "    c. Destination: Identify the next component in the flow (e.g., database, application logic).\n",
    "\n",
    "Step 3: Ending Point\n",
    "3.1 Final Node: Identify the ending point of the flow (consider each and every arrow, box, dotted box, figures, nodes, etc). There can be multiple end / final nodes.\n",
    "3.2 Output/Result: Describe the final outcome or action taken at the end (e.g., display results, data saved).\n",
    "\n",
    "Additional Notes:\n",
    "Mention any decision points or conditional branches in the flow.\n",
    "Briefly describe any loops or repetitions in the process.\n",
    "Include any text labels or annotations present in the diagram for clarity.\n",
    "\n",
    "Remember:\n",
    "Instructions for output format: \n",
    "Maintain a three-word format for each step (Node -> Relation -> Node). \n",
    "Do not miss any steps or anything at all.\n",
    "Describe the flow in a hierarchical manner, following each arrow's direction.\n",
    "Avoid summarizing the entire process at once; provide a step-by-step breakdown.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa7ca1c-1262-4ff5-8e20-e9fa5241ef10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "def get_bedrock_response(input_text, image=None):\n",
    "    # Create a client for Bedrock\n",
    "    bedrock_client = boto3.client('bedrock-runtime')\n",
    "    \n",
    "    # Prepare the request body\n",
    "    max_tokens = 200000\n",
    "    \n",
    "    messages =  [\n",
    "            {\"role\": \"user\", \"content\": input_text}\n",
    "        ]\n",
    "\n",
    "    body=json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"system\": prompting,\n",
    "            \"messages\": messages\n",
    "        }  \n",
    "    )  \n",
    "    #print(prompting)\n",
    "        \n",
    "    # Send the request to AWS Bedrock\n",
    "    #anthropic.claude-3-haiku-20240307-v1:0\n",
    "    #anthropic.claude-3-5-sonnet-20240620-v1:0\n",
    "    response = bedrock_client.invoke_model(\n",
    "        modelId='anthropic.claude-3-sonnet-20240229-v1:0',  # Replace with the Bedrock model you want to use\n",
    "        body=body,\n",
    "        contentType='application/json'\n",
    "    )\n",
    "\n",
    "    # Parse the response\n",
    "    result = json.loads(response.get('body').read())\n",
    "\n",
    "    print(\"***Text Response***\")\n",
    "    print(result)\n",
    "    print(\"*resEnd*\")\n",
    "    # Return the generated content from the response\n",
    "    return result['content'][0]['text']\n",
    "\n",
    "def get_bedrock_image_response(input_text, image=None):\n",
    "    # Create a client for Bedrock\n",
    "    bedrock_client = boto3.client('bedrock-runtime')\n",
    "    \n",
    "    # Prepare the request body\n",
    "    max_tokens = 200000\n",
    "    \n",
    "    messages =  [\n",
    "            {\"role\": \"user\", \"content\": input_text}\n",
    "        ]\n",
    "\n",
    "    body=json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"system\": prompting_image,\n",
    "            \"messages\": messages\n",
    "        }  \n",
    "    )  \n",
    "    #print(prompting)\n",
    "        \n",
    "    # Send the request to AWS Bedrock\n",
    "    #anthropic.claude-3-haiku-20240307-v1:0\n",
    "    #anthropic.claude-3-5-sonnet-20240620-v1:0\n",
    "    response = bedrock_client.invoke_model(\n",
    "        modelId='anthropic.claude-3-sonnet-20240229-v1:0',  # Replace with the Bedrock model you want to use\n",
    "        body=body,\n",
    "        contentType='application/json'\n",
    "    )\n",
    "\n",
    "    # Parse the response\n",
    "    result = json.loads(response.get('body').read())\n",
    "\n",
    "    print(\"***Text Response***\")\n",
    "    print(result)\n",
    "    print(\"*resEnd*\")\n",
    "    # Return the generated content from the response\n",
    "    return result['content'][0]['text']\n",
    "\n",
    "\n",
    "def run_multi_modal_prompt(bedrock_runtime, model_id, messages, max_tokens=200000):\n",
    "    \"\"\"\n",
    "    Invokes a model with a multimodal prompt.\n",
    "    Args:\n",
    "        bedrock_runtime: The Amazon Bedrock boto3 client.\n",
    "        model_id (str): The model ID to use.\n",
    "        messages (JSON) : The messages to send to the model.\n",
    "        max_tokens (int) : The maximum  number of tokens to generate.\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    body = json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": max_tokens,\n",
    "             \"messages\": messages\n",
    "        }\n",
    "    )\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=body, modelId=model_id)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "    return response_body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd85418-ce72-45c9-b512-c2a611bcbfbf",
   "metadata": {},
   "source": [
    "### Downloading the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7e48abae-09fc-4d33-96c1-9d641897b3d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-27 02:23:02--  https://arxiv.org/pdf/2411.16732\n",
      "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.67.42, 151.101.131.42, ...\n",
      "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 567352 (554K) [application/pdf]\n",
      "Saving to: ‘2411.16732’\n",
      "\n",
      "100%[======================================>] 567,352     --.-K/s   in 0.02s   \n",
      "\n",
      "2024-11-27 02:23:02 (30.7 MB/s) - ‘2411.16732’ saved [567352/567352]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O ./data/pdf/paper.pdf \"https://arxiv.org/pdf/2411.16732\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e7b5b0a-f48e-4b84-be05-2bd89a2e6259",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Federal Proposed budget_fy2025 (2).pdf', '10reasons.pdf']\n",
      "11897\n",
      "***Text Response***\n",
      "{'id': 'msg_bdrk_01UhvX1XiXXAysQzDCrHhk31', 'type': 'message', 'role': 'assistant', 'model': 'claude-3-sonnet-20240229', 'content': [{'type': 'text', 'text': \"Here is the list of triples extracted from the given text:\\n\\n['Amazon Bedrock~~offers~~broad selection of FMs', 'Amazon Bedrock~~offers~~choice of high-performing LLMs', 'Amazon Bedrock~~offers~~Claude 3 Opus', 'Amazon Bedrock~~features~~seven model providers', 'Amazon Bedrock~~features~~30+ models', 'Amazon Bedrock~~offers~~security and data governance controls', 'Amazon Bedrock~~encrypts~~data at rest and in transit', 'Amazon Bedrock~~allows~~creation of encryption keys', 'Amazon Bedrock~~offers~~identity-based policies', 'Amazon Bedrock~~complies with~~compliance standards', 'Amazon Bedrock~~preserves~~customer data privacy', 'Amazon Bedrock~~preserves~~data integrity', 'Claude 3 Sonnet~~costs less than~~GPT-4o', 'Claude 3 Sonnet~~costs~~$3 for 1M input tokens', 'Claude 3 Haiku~~costs~~$0.25/1M input tokens', 'Mixtral 8x7B~~costs~~$0.45/1M input tokens', 'Claude 3~~outperforms~~GPT-4', 'Claude 3~~offers~~200K context windows', 'GPT-4o~~offers~~128K context window', 'Claude 3 Haiku~~supports~~2 million tokens per minute', 'Claude 3 Haiku~~processes~~21,000 tokens per second', 'Guardrails for Amazon Bedrock~~blocks~~harmful content', 'Guardrails for Amazon Bedrock~~protects~~content', 'Knowledge Bases~~enables~~Retrieval Augmented Generation', 'Knowledge Bases~~delivers~~relevant responses', 'Knowledge Bases~~delivers~~accurate responses', 'Knowledge Bases~~delivers~~customized responses', 'Agents for Amazon Bedrock~~creates~~prompts', 'Agents for Amazon Bedrock~~includes~~instructions', 'Agents for Amazon Bedrock~~includes~~action groups', 'Agents for Amazon Bedrock~~includes~~knowledge bases', 'Agents for Amazon Bedrock~~includes~~prompt templates', 'Amazon Bedrock Studio~~streamlines~~development process', 'Amazon Bedrock Studio~~enables~~collaboration', 'Amazon Bedrock Studio~~enables~~testing', 'Amazon Bedrock Studio~~enables~~debugging', 'Custom Model Import~~enables~~custom model usage', 'Custom Model Import~~enables~~fine-tuning', 'Custom Model Import~~enables~~continued pretraining', 'Amazon Bedrock~~built on~~AWS cloud', 'AWS cloud~~delivers~~cloud services', 'AWS cloud~~provides~~operational experience']\"}], 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 3711, 'output_tokens': 640}}\n",
      "*resEnd*\n"
     ]
    }
   ],
   "source": [
    "#PDF Based\n",
    "import os\n",
    "import PyPDF2\n",
    "import streamlit as st\n",
    "\n",
    "# Define the path to the directory containing the PDFs\n",
    "\n",
    "pdf_directory = \"./data/pdf\"\n",
    "\n",
    "# List all PDF files in the directory\n",
    "pdf_files = [f for f in os.listdir(pdf_directory) if f.endswith('.pdf')]\n",
    "print(pdf_files)\n",
    "\n",
    "pdf_path = os.path.join(pdf_directory, \"10reasons.pdf\")\n",
    "\n",
    "# Extract text from the PDF file\n",
    "with open(pdf_path, 'rb') as pdf_file:\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "    text_data = \"\"\n",
    "    # Extract text from all pages\n",
    "    for page_num in range(len(pdf_reader.pages)):\n",
    "        page = pdf_reader.pages[page_num]\n",
    "        text_data += page.extract_text()\n",
    "\n",
    "print(len(text_data))\n",
    "    # Call the Bedrock model using the extracted text (image parameter is removed)\n",
    "if text_data:\n",
    "    response_text = get_bedrock_response(text_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31f0e616-ac9c-47b5-848b-28843a12983f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gremlin_python.structure.graph import Graph\n",
    "from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection\n",
    "from gremlin_python.process.graph_traversal import unfold, addV\n",
    "import pandas as pd\n",
    "import ast\n",
    "import nest_asyncio\n",
    "\n",
    "from gremlin_python.process.graph_traversal import __\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "#Knowledge Graph from Text\n",
    "def KG_from_Text(user_query, table_name):\n",
    "    #print(user_query)\n",
    "    a1 = get_bedrock_response(user_query)\n",
    "    phrase_to_remove = \"Here is the list of key information extracted from the document in the requested format:\\n\\n\"\n",
    "    #if a1.startswith(phrase_to_remove):\n",
    "    a1 = a1.split(\"\\n\\n\", 1)[1]\n",
    "    print(\"res\", a1)\n",
    "    df_list = ast.literal_eval(a1)\n",
    "    split_data = [item.split('~~', 2) for item in df_list]\n",
    "    df = pd.DataFrame(split_data, columns=['Source_Node', 'Relationship', 'Target_Node'])\n",
    "    print(df.head(10))\n",
    "    #pushingIntoDB = dfIntoDB(df, table_name)\n",
    " \n",
    "    graph = Graph()\n",
    "    connection = DriverRemoteConnection('wss://neptunedbcluster-vugbwk7upvfa.cluster-cdpzhcjwte6u.us-east-1.neptune.amazonaws.com:8182/gremlin', 'g')\n",
    "    g = graph.traversal().withRemote(connection)\n",
    "\n",
    "    # Push data into Neptune\n",
    "    for index, row in df.iterrows():\n",
    "        source_node = row['Source_Node']\n",
    "        relationship_type = row['Relationship']\n",
    "        target_node = row['Target_Node']\n",
    "\n",
    "        g.V().has('name', source_node).fold().coalesce(\n",
    "            __.unfold(),\n",
    "            __.addV('Node').property('name', source_node)\n",
    "        ).next()\n",
    "\n",
    "# Similarly, for the target node\n",
    "        g.V().has('name', target_node).fold().coalesce(\n",
    "            __.unfold(),\n",
    "            __.addV('Node').property('name', target_node)\n",
    "        ).next()\n",
    "\n",
    "# Creating an edge/relationship between the source and target nodes\n",
    "        g.V().has('name', source_node).addE(relationship_type).to(\n",
    "            __.V().has('name', target_node)  # Use __ instead of g here\n",
    "        ).next()\n",
    "    \n",
    "\n",
    "    print(\"Data imported into Neptune graph database successfully.\")\n",
    "    result = \"Data imported into Neptune graph database successfully.\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ce579fe-c55d-47e6-b318-c025cdaadb6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gremlin_python.structure.graph import Graph\n",
    "from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection\n",
    "from gremlin_python.process.graph_traversal import unfold, addV\n",
    "import pandas as pd\n",
    "import ast\n",
    "import nest_asyncio\n",
    "\n",
    "from gremlin_python.process.graph_traversal import __\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "#Knowledge Graph from Text\n",
    "def KG_from_Text_Image(user_query, table_name):\n",
    "    #print(user_query)\n",
    "    a1 = get_bedrock_image_response(user_query)\n",
    "    phrase_to_remove = \"Here is the list of key information extracted from the document in the requested format:\\n\\n\"\n",
    "    #if a1.startswith(phrase_to_remove):\n",
    "    a1 = a1.split(\"\\n\\n\", 1)[1]\n",
    "    print(\"res\", a1)\n",
    "    df_list = ast.literal_eval(a1)\n",
    "    split_data = [item.split('~~', 2) for item in df_list]\n",
    "    df = pd.DataFrame(split_data, columns=['Source_Node', 'Relationship', 'Target_Node'])\n",
    "    print(df.head(10))\n",
    "    #pushingIntoDB = dfIntoDB(df, table_name)\n",
    " \n",
    "    graph = Graph()\n",
    "    connection = DriverRemoteConnection('wss://neptunedbcluster-vugbwk7upvfa.cluster-cdpzhcjwte6u.us-east-1.neptune.amazonaws.com:8182/gremlin', 'g')\n",
    "    g = graph.traversal().withRemote(connection)\n",
    "\n",
    "    # Push data into Neptune\n",
    "    for index, row in df.iterrows():\n",
    "        source_node = row['Source_Node']\n",
    "        relationship_type = row['Relationship']\n",
    "        target_node = row['Target_Node']\n",
    "\n",
    "        g.V().has('name', source_node).fold().coalesce(\n",
    "            __.unfold(),\n",
    "            __.addV('Node').property('name', source_node)\n",
    "        ).next()\n",
    "\n",
    "# Similarly, for the target node\n",
    "        g.V().has('name', target_node).fold().coalesce(\n",
    "            __.unfold(),\n",
    "            __.addV('Node').property('name', target_node)\n",
    "        ).next()\n",
    "\n",
    "# Creating an edge/relationship between the source and target nodes\n",
    "        g.V().has('name', source_node).addE(relationship_type).to(\n",
    "            __.V().has('name', target_node)  # Use __ instead of g here\n",
    "        ).next()\n",
    "    \n",
    "\n",
    "    print(\"Data imported into Neptune graph database successfully.\")\n",
    "    result = \"Data imported into Neptune graph database successfully.\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03c3f7e3-b443-4baa-b036-b64c438189c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available images:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Prompt the user to select an image by number\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m selected_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mSelect an image by number: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selected_index \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m selected_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(image_files):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid selection.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "#Image Based\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Define the path to the directory containing the images\n",
    "image_directory = \"./data/images\"\n",
    "\n",
    "# List all image files (jpg, jpeg, png) in the directory\n",
    "image_files = [f for f in os.listdir(image_directory) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "# Display the list of image files for the user to select\n",
    "print(\"Available images:\")\n",
    "for idx, image_file in enumerate(image_files):\n",
    "    print(f\"{idx + 1}: {image_file}\")\n",
    "\n",
    "# Prompt the user to select an image by number\n",
    "selected_index = int(input(\"\\nSelect an image by number: \")) - 1\n",
    "if selected_index < 0 or selected_index >= len(image_files):\n",
    "    print(\"Invalid selection.\")\n",
    "    exit()\n",
    "\n",
    "# Construct the full path to the selected image\n",
    "selected_image = image_files[selected_index]\n",
    "image_path = os.path.join(image_directory, selected_image)\n",
    "\n",
    "# Load the image using Pillow\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Display the selected image file name\n",
    "print(f\"\\nSelected Image: {selected_image}\")\n",
    "\n",
    "# Optionally, get some input text from the user\n",
    "input_text = input(\"\\nEnter any text input to send to the model (leave blank if none): \")\n",
    "\n",
    "# Call the Bedrock model using the image and optional text input\n",
    "response_text = get_bedrock_response(input_text, image)\n",
    "\n",
    "# Display the response from the Bedrock model\n",
    "print(\"\\n***Response from Bedrock Model***\")\n",
    "print(response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5b0618-db0e-4788-b96a-a2f7b457bba3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call the KG_from_Text function to process \n",
    "KG_from_Text(text_data, \"index_111\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3875c15b-a692-42fe-b3c0-e9a68edfe50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_user_query_neptune = \"\"\"Your task is to refine the user query for spelling mistakes and syntax errors according to the Table Description part below. Do not add any information on your own in the refined query, just check for spellings and correct them according to the words in Table Description given below.\n",
    "\n",
    "The user query to be refined is: {user_input}\n",
    "\n",
    "Table Description for Correct syntax of Cypher query to be generated: \n",
    "{df_string}\n",
    "\n",
    "\n",
    "Output Format:\n",
    "The output must be a simple sentence made in accordance to the user query with correct syntax according to the given words.\n",
    "If the user query is already having correct syntax according to the Table Description, then just return the user query itself without any refinement and do not provide any extra sentence.\n",
    "Do not provide any extra sentence.\n",
    "\"\"\"\n",
    "\n",
    "pr2 = \"\"\"\n",
    "Your task is to create a Gremlin query to be run in Amazon Neptune.\n",
    "\n",
    "Refer to this table description and the given examples to generate Gremlin queries with appropriate syntax. The table contains source node, relation, and target node information, where each row represents a relationship between From Asset and To Asset.\n",
    "\n",
    "Table Description for Correct Syntax of Gremlin Query to Be Generated:\n",
    "{df_string}\n",
    "\n",
    "Given examples for Reference\n",
    "Use these examples to understand the Gremlin query syntax. Always use has(\"name\", \"node_name\") for specifying node names.\n",
    "\n",
    "Example 1:\n",
    "User Query: \"Give downstream lineage of node_name.\"\n",
    "Generated Gremlin Query: g.V().has(\"name\", \"node_name\").repeat(out()).path().by(valueMap()).limit(10)\n",
    "Example 2:\n",
    "User Query: \"Give any information about node_name.\"\n",
    "Generated Gremlin Query: g.V().has(\"name\", \"node_name\").bothE().otherV().path().by(valueMap())\n",
    "Example 3:\n",
    "User Query: \"Are node_name1 and node_name2 connected?\"\n",
    "Generated Gremlin Query: g.V().has(\"name\", \"node_name1\").repeat(both()).until(has(\"name\", \"node_name2\")).path().by(valueMap())\n",
    "\n",
    "Generate the Gremlin query for the user_query below\n",
    "{user_input}\n",
    "\n",
    "*Attention*: Only provide the generated Gremlin query as output with no extra words or information.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "NLP = \"\"\"\n",
    "Given a user_query: \n",
    "{user_input} \n",
    "\n",
    "and its output response: \n",
    "{response0}\n",
    "\n",
    "Use the user_query and its output response given above to generate output in simple English without missing any point.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bd15d59e-c203-4108-8c2b-0224034c83d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from gremlin_python.driver import client, serializer\n",
    "\n",
    "def shubhNeptune(user_query, table_name):\n",
    "    bedrock_client = boto3.client('bedrock-runtime')\n",
    "    \n",
    "    # Prepare the request body\n",
    "    max_tokens = 400\n",
    "    \n",
    "    messages =  [\n",
    "            {\"role\": \"user\", \"content\": user_query}\n",
    "        ]\n",
    "\n",
    "    body=json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"system\": pr2,\n",
    "            \"messages\": messages\n",
    "        }  \n",
    "    )  \n",
    "    \n",
    "    response = bedrock_client.invoke_model(\n",
    "        modelId='anthropic.claude-3-sonnet-20240229-v1:0',  # Replace with the Bedrock model you want to use\n",
    "        body=body,\n",
    "        contentType='application/json'\n",
    "    )\n",
    "\n",
    "    # Your Neptune endpoint and port\n",
    "    neptune_endpoint = 'neptunedbcluster-vugbwk7upvfa.cluster-cdpzhcjwte6u.us-east-1.neptune.amazonaws.com'\n",
    "    neptune_port = '8182'\n",
    "\n",
    "    # Create a Neptune client\n",
    "    neptune_client = client.Client(\n",
    "        f'wss://{neptune_endpoint}:{neptune_port}/gremlin',\n",
    "        'g',\n",
    "        message_serializer=serializer.GraphSONSerializersV2d0()\n",
    "    )\n",
    "    \n",
    "    # Parse the response\n",
    "    result = json.loads(response.get('body').read())\n",
    "\n",
    "    #print(\"***Text Response***\")\n",
    "    #print(result)\n",
    "    #print(\"*resEnd*\")\n",
    "\n",
    "    rag_result = neptune_client.submit(result['content'][0]['text']).all().result()\n",
    "    #print(rag_result)\n",
    "    \n",
    "    # Initialize a list to store the extracted relationships\n",
    "    relationships = []\n",
    "\n",
    "    # Iterate over each path\n",
    "    for path in rag_result:\n",
    "        temp = []\n",
    "        for element in path:\n",
    "            #print(element)\n",
    "            temp.append(element)\n",
    "        relationships.append((temp[0]['name'][0], temp[2]['name'][0]))\n",
    "\n",
    "    # Print the extracted relationships\n",
    "    rag_resultparse = \"\"\n",
    "    for relationship in relationships:\n",
    "        rag_resultparse += f\"Source Node: {relationship[0]}, Target Node: {relationship[1]}\"\n",
    "    \n",
    "    user_input = \"user_input: \\n\" + user_query + \"\\n response0: \\n\" + rag_resultparse\n",
    "    messages =  [\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    \n",
    "    \n",
    "    body=json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"system\": NLP,\n",
    "            \"messages\": messages\n",
    "        }  \n",
    "    )  \n",
    "    \n",
    "    response = bedrock_client.invoke_model(\n",
    "        modelId='anthropic.claude-3-sonnet-20240229-v1:0',  # Replace with the Bedrock model you want to use\n",
    "        body=body,\n",
    "        contentType='application/json'\n",
    "    )\n",
    "\n",
    "    # Parse the response\n",
    "    result = json.loads(response.get('body').read())\n",
    "\n",
    "    #print(\"***Text Response***\")\n",
    "    #print(result)\n",
    "    #print(\"*resEnd*\")\n",
    "    # Return the generated content from the response\n",
    "    return result['content'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fba0347c-ba65-486c-a2cc-a2f673bdd942",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What does AmazonBedrock have\"\n",
    "\n",
    "result = \"\"\n",
    "\n",
    "try:\n",
    "    result = shubhNeptune(user_query, \"index_111\") # Replace with your function call\n",
    "except Exception as e:\n",
    "    result = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7d30628-42cb-4cfd-8821-5c103bd19700",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresult\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc99c168-9143-4c8d-8e5e-73cef78c6026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"msg_bdrk_01VmfGvxcNfFLrGRCsA31dtq\",\n",
      "    \"type\": \"message\",\n",
      "    \"role\": \"assistant\",\n",
      "    \"model\": \"claude-3-sonnet-20240229\",\n",
      "    \"content\": [\n",
      "        {\n",
      "            \"type\": \"text\",\n",
      "            \"text\": \"This image presents a diagram illustrating the architecture of a type of neural network model, likely related to natural language processing tasks. The model appears to be an encoder-decoder architecture with multi-head attention mechanisms.\\n\\nOn the left side, the diagram shows the encoder component, which takes inputs and passes them through an input embedding layer, followed by positional encoding. The encoded representations then go through multiple layers of feed-forward networks, multi-head attention layers, and add & norm operations.\\n\\nOn the right side, the diagram depicts the decoder component, which follows a similar structure with feed-forward networks, multi-head attention layers (both regular and masked), and add & norm operations. The decoder also receives positional encoding inputs.\\n\\nThe outputs from the decoder component go through a linear layer and a softmax operation to produce output probabilities, which could be used for tasks such as language generation or translation.\\n\\nOverall, this diagram illustrates the high-level architecture of a transformer-based neural network model, commonly used in natural language processing applications.\"\n",
      "        }\n",
      "    ],\n",
      "    \"stop_reason\": \"end_turn\",\n",
      "    \"stop_sequence\": null,\n",
      "    \"usage\": {\n",
      "        \"input_tokens\": 1553,\n",
      "        \"output_tokens\": 217\n",
      "    }\n",
      "}\n",
      "***Text Response***\n",
      "{'id': 'msg_bdrk_013cQRqEkSRZKSqrZSqAVRDj', 'type': 'message', 'role': 'assistant', 'model': 'claude-3-sonnet-20240229', 'content': [{'type': 'text', 'text': \"Here is the list of node relationships extracted from the given data:\\n\\n['Input~~passedTo~~InputEmbeddingLayer', 'InputEmbeddingLayer~~outputTo~~PositionalEncoding', 'PositionalEncoding~~inputTo~~EncoderLayer', 'EncoderLayer~~contains~~FeedForwardNetwork', 'EncoderLayer~~contains~~MultiHeadAttentionLayer', 'EncoderLayer~~contains~~AddNormOperation', 'EncoderLayer~~outputTo~~DecoderLayer', 'DecoderLayer~~contains~~FeedForwardNetwork', 'DecoderLayer~~contains~~MultiHeadAttentionLayer', 'DecoderLayer~~contains~~MaskedMultiHeadAttentionLayer', 'DecoderLayer~~contains~~AddNormOperation', 'DecoderLayer~~inputTo~~PositionalEncoding', 'DecoderLayer~~outputTo~~LinearLayer', 'LinearLayer~~outputTo~~SoftmaxOperation', 'SoftmaxOperation~~produces~~OutputProbabilities', 'OutputProbabilities~~usedFor~~LanguageGeneration', 'OutputProbabilities~~usedFor~~Translation']\"}], 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 921, 'output_tokens': 281}}\n",
      "*resEnd*\n",
      "res ['Input~~passedTo~~InputEmbeddingLayer', 'InputEmbeddingLayer~~outputTo~~PositionalEncoding', 'PositionalEncoding~~inputTo~~EncoderLayer', 'EncoderLayer~~contains~~FeedForwardNetwork', 'EncoderLayer~~contains~~MultiHeadAttentionLayer', 'EncoderLayer~~contains~~AddNormOperation', 'EncoderLayer~~outputTo~~DecoderLayer', 'DecoderLayer~~contains~~FeedForwardNetwork', 'DecoderLayer~~contains~~MultiHeadAttentionLayer', 'DecoderLayer~~contains~~MaskedMultiHeadAttentionLayer', 'DecoderLayer~~contains~~AddNormOperation', 'DecoderLayer~~inputTo~~PositionalEncoding', 'DecoderLayer~~outputTo~~LinearLayer', 'LinearLayer~~outputTo~~SoftmaxOperation', 'SoftmaxOperation~~produces~~OutputProbabilities', 'OutputProbabilities~~usedFor~~LanguageGeneration', 'OutputProbabilities~~usedFor~~Translation']\n",
      "           Source_Node Relationship                    Target_Node\n",
      "0                Input     passedTo            InputEmbeddingLayer\n",
      "1  InputEmbeddingLayer     outputTo             PositionalEncoding\n",
      "2   PositionalEncoding      inputTo                   EncoderLayer\n",
      "3         EncoderLayer     contains             FeedForwardNetwork\n",
      "4         EncoderLayer     contains        MultiHeadAttentionLayer\n",
      "5         EncoderLayer     contains               AddNormOperation\n",
      "6         EncoderLayer     outputTo                   DecoderLayer\n",
      "7         DecoderLayer     contains             FeedForwardNetwork\n",
      "8         DecoderLayer     contains        MultiHeadAttentionLayer\n",
      "9         DecoderLayer     contains  MaskedMultiHeadAttentionLayer\n",
      "Data imported into Neptune graph database successfully.\n",
      "{\n",
      "    \"id\": \"msg_bdrk_01DUNA4jp5255oMwqWuKqE94\",\n",
      "    \"type\": \"message\",\n",
      "    \"role\": \"assistant\",\n",
      "    \"model\": \"claude-3-sonnet-20240229\",\n",
      "    \"content\": [\n",
      "        {\n",
      "            \"type\": \"text\",\n",
      "            \"text\": \"This image depicts a flow diagram or process chart. It consists of rectangular boxes connected by arrows, representing different steps or components in a sequence or system.\\n\\nThe top box is labeled \\\"MatMul\\\", which likely stands for matrix multiplication operation.\\n\\nThe next box below is \\\"SoftMax\\\", which is a type of activation function commonly used in machine learning models.\\n\\nThe optional box below is labeled \\\"Mask (opt.)\\\", suggesting some kind of masking operation that is optional.\\n\\nThe next box is \\\"Scale\\\", which could refer to a scaling or normalization step.\\n\\nAnother \\\"MatMul\\\" box appears at the bottom, suggesting another matrix multiplication operation.\\n\\nThe bottom labels are single letters \\\"Q\\\", \\\"K\\\", and \\\"V\\\", which may represent some inputs or components, but I cannot infer their specific meanings from this image alone.\\n\\nOverall, this diagram seems to outline a series of operations or layers, possibly within a machine learning or neural network architecture, involving matrix multiplications, softmax activation, optional masking, scaling, and some inputs represented by the letters.\"\n",
      "        }\n",
      "    ],\n",
      "    \"stop_reason\": \"end_turn\",\n",
      "    \"stop_sequence\": null,\n",
      "    \"usage\": {\n",
      "        \"input_tokens\": 529,\n",
      "        \"output_tokens\": 226\n",
      "    }\n",
      "}\n",
      "***Text Response***\n",
      "{'id': 'msg_bdrk_01JEjc2v93dyEfNfhK8qAVGo', 'type': 'message', 'role': 'assistant', 'model': 'claude-3-sonnet-20240229', 'content': [{'type': 'text', 'text': \"Thank you for the detailed description of the image. Based on the information provided, here is the list in the required format:\\n\\n['MatMul~~is~~Operation', 'SoftMax~~is~~ActivationFunction', 'Mask~~is~~OptionalOperation', 'Scale~~is~~Operation', 'MatMul~~is~~Operation', 'Q~~is~~Input', 'K~~is~~Input', 'V~~is~~Input']\"}], 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 930, 'output_tokens': 104}}\n",
      "*resEnd*\n",
      "res ['MatMul~~is~~Operation', 'SoftMax~~is~~ActivationFunction', 'Mask~~is~~OptionalOperation', 'Scale~~is~~Operation', 'MatMul~~is~~Operation', 'Q~~is~~Input', 'K~~is~~Input', 'V~~is~~Input']\n",
      "  Source_Node Relationship         Target_Node\n",
      "0      MatMul           is           Operation\n",
      "1     SoftMax           is  ActivationFunction\n",
      "2        Mask           is   OptionalOperation\n",
      "3       Scale           is           Operation\n",
      "4      MatMul           is           Operation\n",
      "5           Q           is               Input\n",
      "6           K           is               Input\n",
      "7           V           is               Input\n",
      "Data imported into Neptune graph database successfully.\n",
      "{\n",
      "    \"id\": \"msg_bdrk_014Js8gcbKw6aSqsGCBu8w3E\",\n",
      "    \"type\": \"message\",\n",
      "    \"role\": \"assistant\",\n",
      "    \"model\": \"claude-3-sonnet-20240229\",\n",
      "    \"content\": [\n",
      "        {\n",
      "            \"type\": \"text\",\n",
      "            \"text\": \"This image depicts the architecture or structure of a Scaled Dot-Product Attention mechanism, which is a key component in Transformer models used for various natural language processing tasks.\\n\\nThe diagram shows that the Scaled Dot-Product Attention block takes inputs V, K, and Q, which likely represent the Value, Key, and Query vectors respectively. These inputs go through separate Linear layers to project them into appropriate representations.\\n\\nThe projected representations are then fed into the Scaled Dot-Product Attention block, which computes the attention scores by taking the dot product of the Query and Key vectors, scaling the result, and applying a softmax function to obtain the weights. These weights are then used to compute a weighted sum of the Value vectors, producing the attended output.\\n\\nThe attended output from the Scaled Dot-Product Attention block is then concatenated and passed through another Linear layer to produce the final output representation denoted by \\\"h\\\".\\n\\nOverall, this diagram illustrates the multi-head attention mechanism that allows the model to attend to different parts of the input sequence when computing representations for sequence-to-sequence or other language tasks.\"\n",
      "        }\n",
      "    ],\n",
      "    \"stop_reason\": \"end_turn\",\n",
      "    \"stop_sequence\": null,\n",
      "    \"usage\": {\n",
      "        \"input_tokens\": 1397,\n",
      "        \"output_tokens\": 239\n",
      "    }\n",
      "}\n",
      "***Text Response***\n",
      "{'id': 'msg_bdrk_01WBJrGjq2uYW2tWMWxrbeRp', 'type': 'message', 'role': 'assistant', 'model': 'claude-3-sonnet-20240229', 'content': [{'type': 'text', 'text': \"Here is the list of triples extracted from the given text:\\n\\n['Attention_mechanism~~takes_inputs~~Value_Vector', 'Attention_mechanism~~takes_inputs~~Key_Vector', 'Attention_mechanism~~takes_inputs~~Query_Vector', 'Value_Vector~~goes_through~~Linear_layer', 'Key_Vector~~goes_through~~Linear_layer', 'Query_Vector~~goes_through~~Linear_layer', 'Scaled_Dot-Product_Attention~~computes~~attention_scores', 'attention_scores~~computed_by~~dot_product_of_Query_and_Key', 'attention_scores~~scaled~~scaling', 'attention_scores~~transformed_by~~softmax', 'softmax~~outputs~~weights', 'weights~~used_for~~weighted_sum_of_Value_vectors', 'weighted_sum_of_Value_vectors~~produces~~attended_output', 'attended_output~~concatenated_with~~other_attended_outputs', 'concatenated_attended_outputs~~passed_through~~Linear_layer', 'Linear_layer~~produces~~final_output_representation']\"}], 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 943, 'output_tokens': 261}}\n",
      "*resEnd*\n",
      "res ['Attention_mechanism~~takes_inputs~~Value_Vector', 'Attention_mechanism~~takes_inputs~~Key_Vector', 'Attention_mechanism~~takes_inputs~~Query_Vector', 'Value_Vector~~goes_through~~Linear_layer', 'Key_Vector~~goes_through~~Linear_layer', 'Query_Vector~~goes_through~~Linear_layer', 'Scaled_Dot-Product_Attention~~computes~~attention_scores', 'attention_scores~~computed_by~~dot_product_of_Query_and_Key', 'attention_scores~~scaled~~scaling', 'attention_scores~~transformed_by~~softmax', 'softmax~~outputs~~weights', 'weights~~used_for~~weighted_sum_of_Value_vectors', 'weighted_sum_of_Value_vectors~~produces~~attended_output', 'attended_output~~concatenated_with~~other_attended_outputs', 'concatenated_attended_outputs~~passed_through~~Linear_layer', 'Linear_layer~~produces~~final_output_representation']\n",
      "                    Source_Node    Relationship                   Target_Node\n",
      "0           Attention_mechanism    takes_inputs                  Value_Vector\n",
      "1           Attention_mechanism    takes_inputs                    Key_Vector\n",
      "2           Attention_mechanism    takes_inputs                  Query_Vector\n",
      "3                  Value_Vector    goes_through                  Linear_layer\n",
      "4                    Key_Vector    goes_through                  Linear_layer\n",
      "5                  Query_Vector    goes_through                  Linear_layer\n",
      "6  Scaled_Dot-Product_Attention        computes              attention_scores\n",
      "7              attention_scores     computed_by  dot_product_of_Query_and_Key\n",
      "8              attention_scores          scaled                       scaling\n",
      "9              attention_scores  transformed_by                       softmax\n",
      "Data imported into Neptune graph database successfully.\n"
     ]
    }
   ],
   "source": [
    "#Image Based\n",
    "import os\n",
    "from pypdf import PdfReader\n",
    "import PyPDF2\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "\n",
    "bedrock_runtime = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "reader = PdfReader(\"transformers.pdf\")\n",
    "\n",
    "for page in reader.pages:\n",
    "    for i in page.images:\n",
    "       \n",
    "        \n",
    "        # print(type(x))\n",
    "        buffer = io.BytesIO()\n",
    "        i.image.save(buffer, format='PNG', quality=75)\n",
    "        imageObject = buffer.getbuffer()\n",
    "        \n",
    "        content_image = base64.b64encode(imageObject).decode('utf8')\n",
    "        \n",
    "        message = {\"role\": \"user\",\n",
    "                 \"content\": [\n",
    "                    {\"type\": \"image\", \"source\": {\"type\": \"base64\",\n",
    "                        \"media_type\": \"image/jpeg\", \"data\": content_image}},\n",
    "                    {\"type\": \"text\", \"text\": \"What's in this image?\"}\n",
    "                ]}\n",
    "        \n",
    "        messages = [message]\n",
    "        response = run_multi_modal_prompt(\n",
    "        bedrock_runtime, 'anthropic.claude-3-sonnet-20240229-v1:0', messages)\n",
    "        \n",
    "        print(json.dumps(response, indent=4))\n",
    "\n",
    "        KG_from_Text_Image(response[\"content\"][0][\"text\"], \"index_115\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7528083c-e109-4192-989a-761618ce3705",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a simple explanation in English of what SoftMax has, based on the given response:\n",
      "\n",
      "SoftMax is a type of activation function used in neural networks. It is also a block or component that can be part of a computational flow diagram or pipeline. SoftMax can be used for masking or selectively allowing/blocking certain outputs. It is sometimes used in conjunction with the matrix multiplication operation (MatMul) in neural network architectures.\n",
      "\n",
      "In summary, SoftMax is a mathematical function commonly employed in neural networks for various purposes like activation, masking, and normalization as part of the overall network architecture and data flow.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"What does SoftMax have\"\n",
    "\n",
    "result = \"\"\n",
    "\n",
    "try:\n",
    "    result = shubhNeptune(user_query, \"index_115\") # Replace with your function call\n",
    "except Exception as e:\n",
    "    result = e\n",
    "    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ed11706-e5b2-4094-b686-7adefe399380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean code and add Neptune RAG for Images\n",
    "\n",
    "# Traceback (most recent call last):\n",
    "\n",
    "#   File ~/anaconda3/envs/JupyterSystemEnv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3460 in run_code\n",
    "#     exec(code_obj, self.user_global_ns, self.user_ns)\n",
    "\n",
    "#   Cell In[28], line 37\n",
    "#     KG_from_Text(response[\"content\"][0][\"text\"], \"index_115\")\n",
    "\n",
    "#   Cell In[18], line 20 in KG_from_Text\n",
    "#     df_list = ast.literal_eval(a1)\n",
    "\n",
    "#   File ~/anaconda3/envs/JupyterSystemEnv/lib/python3.10/ast.py:64 in literal_eval\n",
    "#     node_or_string = parse(node_or_string.lstrip(\" \\t\"), mode='eval')\n",
    "\n",
    "#   File ~/anaconda3/envs/JupyterSystemEnv/lib/python3.10/ast.py:50 in parse\n",
    "#     return compile(source, filename, mode, flags,\n",
    "\n",
    "#   File <unknown>:1\n",
    "#     1. MatMul - This performs matrix multiplication, likely between the query (Q) and key (K) vectors to obtain the attention scores.\n",
    "#        ^\n",
    "# SyntaxError: invalid syntax"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
